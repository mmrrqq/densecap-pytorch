{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from model.densecap import densecap_resnet50_fpn, DenseCapModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_config_path: Path, checkpoint_path: Path, return_features=False, box_per_img=50, verbose=False, token_to_idx=None):\n",
    "    with open(model_config_path, 'r') as f:\n",
    "        model_args = json.load(f)\n",
    "\n",
    "    model = densecap_resnet50_fpn(backbone_pretrained=model_args['backbone_pretrained'],\n",
    "                                  return_features=return_features,\n",
    "                                  feat_size=model_args['feat_size'],\n",
    "                                  hidden_size=model_args['hidden_size'],\n",
    "                                  max_len=model_args['max_len'],\n",
    "                                  emb_size=model_args['emb_size'],\n",
    "                                  rnn_num_layers=model_args['rnn_num_layers'],\n",
    "                                  vocab_size=model_args['vocab_size'],\n",
    "                                  fusion_type=model_args['fusion_type'],\n",
    "                                  box_detections_per_img=box_per_img,\n",
    "                                  token_to_idx=token_to_idx)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    if verbose and 'results_on_val' in checkpoint.keys():\n",
    "        print('[INFO]: checkpoint {} loaded'.format(checkpoint_path))\n",
    "        print('[INFO]: correspond performance on val set:')\n",
    "        for k, v in checkpoint['results_on_val'].items():\n",
    "            if not isinstance(v, dict):\n",
    "                print('        {}: {:.3f}'.format(k, v))\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_image_paths(parent_folder: Path) -> List[str]:\n",
    "    image_paths = []\n",
    "\n",
    "    for child in parent_folder.iterdir():\n",
    "        if child.is_dir():\n",
    "            image_paths.extend(get_image_paths(child))\n",
    "            continue\n",
    "        image_paths.append(str(child))\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def img_to_tensor(img_list, device):\n",
    "    img_tensors = []\n",
    "\n",
    "    for img_path in img_list:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensors.append(tf.ToTensor()(img).to(device))\n",
    "\n",
    "    return img_tensors\n",
    "\n",
    "\n",
    "def describe_images(model: DenseCapModel, img_list: List[str], device: torch.device):\n",
    "    all_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        for i in tqdm(range(0, len(img_list), BATCH_SIZE)):\n",
    "            image_tensors = img_to_tensor(img_list[i:i+BATCH_SIZE], device=device)\n",
    "\n",
    "            results = model(image_tensors)\n",
    "\n",
    "            all_results.extend([{k:v.cpu() for k,v in r.items()} for r in results])\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_results(results, img_paths: List[str], idx_to_token):\n",
    "    results_dict = {}\n",
    "\n",
    "    for img_path, result in zip(img_paths, results):\n",
    "        results_dict[img_path] = []        \n",
    "\n",
    "        for box, cap, score in zip(result['boxes'], result['caps'], result['scores']):            \n",
    "            r = {\n",
    "                'box': [round(c, 2) for c in box.tolist()],\n",
    "                'score': round(score.item(), 2),\n",
    "                'cap': ' '.join(idx_to_token[idx] for idx in cap.tolist()\n",
    "                                if idx_to_token[idx] not in ['<pad>', '<bos>', '<eos>'])\n",
    "            }            \n",
    "\n",
    "            results_dict[img_path].append(r)\n",
    "\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_result(image_file_path, result, idx_to_token=None):\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "    assert isinstance(result, list)\n",
    "\n",
    "    img = Image.open(image_file_path)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    N = 0\n",
    "    for r in result:        \n",
    "        if N > 5:\n",
    "            break\n",
    "        \n",
    "        if idx_to_token is not None:\n",
    "            r['cap'] = ' '.join(idx_to_token[idx] for idx in r['cap'].tolist() if idx_to_token[idx] not in ['<pad>', '<bos>', '<eos>'])        \n",
    "        \n",
    "        if \"car\" not in r['cap']:\n",
    "            continue\n",
    "\n",
    "        N += 1\n",
    "\n",
    "        ax.add_patch(Rectangle((r['box'][0], r['box'][1]),\n",
    "                               r['box'][2]-r['box'][0],\n",
    "                               r['box'][3]-r['box'][1],\n",
    "                               fill=False,\n",
    "                               edgecolor='red',\n",
    "                               linewidth=3))\n",
    "        ax.text(r['box'][0], r['box'][1], r['cap'] + (r['view'] if 'view' in r else \"\"), style='italic', bbox={'facecolor':'white', 'alpha':0.7, 'pad':10})\n",
    "    fig = plt.gcf()\n",
    "    plt.tick_params(labelbottom='off', labelleft='off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: checkpoint compute_model_params/without_aux.pth.tar loaded\n",
      "[INFO]: correspond performance on val set:\n",
      "        map: 0.108\n",
      "        detmap: 0.264\n",
      "../snare/amt/folds_adversarial/train.json\n",
      "Loaded Entries. train: 39278 entries\n",
      "pos model\n",
      "caption loss: torch.Size([8000, 16])\n",
      "box features: torch.Size([8000, 4096])\n",
      "min loss: 10.089165687561035\n",
      "box features at min index: torch.Size([4096])\n",
      "tensor([[  1,   5,  56, 109,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0]])\n",
      "neg model\n",
      "caption loss: torch.Size([8000, 16])\n",
      "box features: torch.Size([8000, 4096])\n",
      "min loss: 10.571321487426758\n",
      "box features at min index: torch.Size([4096])\n",
      "tensor([[ 1,  4, 26,  8, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "wooden chair with cushions\n"
     ]
    }
   ],
   "source": [
    "from utils.snare_dataset import SnareDataset\n",
    "\n",
    "\n",
    "lut_path = Path(\"./data/VG-regions-dicts-lite.pkl\")\n",
    "\n",
    "with open(lut_path, 'rb') as f:\n",
    "    look_up_tables = pickle.load(f)\n",
    "\n",
    "idx_to_token = look_up_tables['idx_to_token']\n",
    "token_to_idx = look_up_tables['token_to_idx']\n",
    "\n",
    "params_path = Path(\"compute_model_params\")\n",
    "model_name = \"without_aux\"\n",
    "model = load_model(\n",
    "    params_path / model_name / \"config.json\", \n",
    "    params_path / (model_name + \".pth.tar\"), \n",
    "    return_features=False, verbose=True, token_to_idx=token_to_idx)\n",
    "\n",
    "\n",
    "dataset = SnareDataset(mode=\"train\")\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "view_ids = torch.arange(8)\n",
    "model.rpn.training = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:    \n",
    "        (key1_imgs, key2_imgs), gt_idx, (key1, key2), annotation, is_visual = batch    \n",
    "        key1_imgs = [k.squeeze() for k in key1_imgs]\n",
    "        key2_imgs = [k.squeeze() for k in key2_imgs]\n",
    "        annotation = annotation[0]\n",
    "        model.train()\n",
    "        model.rpn.training = False\n",
    "        \n",
    "        if gt_idx > 0:\n",
    "            key1_imgs, key2_imgs = key2_imgs, key1_imgs\n",
    "        \n",
    "        print(\"pos model\")\n",
    "        losses, results = model.query_caption(key1_imgs, [annotation], view_ids)\n",
    "        print(\"neg model\")\n",
    "        losses, results = model.query_caption(key2_imgs, [annotation], view_ids)\n",
    "        # decoded_results = postprocess_results(results, [f\"{key1[0]}-{i}\" for i in range(6, 14)], idx_to_token)        \n",
    "        # print(decoded_results)\n",
    "        # losses, results = model.query_caption(key2_imgs, [annotation], view_ids)\n",
    "        # print(losses)\n",
    "        # print(entry_idx)    \n",
    "        print(annotation)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "the\n",
      "wall\n",
      "is\n",
      "white\n",
      "<eos>\n",
      "<bos>\n",
      "a\n",
      "wooden\n",
      "bench\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "x = [ 1,  4, 26,  8, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "\n",
    "for i in x:\n",
    "    if i < 1:\n",
    "        break\n",
    "    print(idx_to_token[i])\n",
    "\n",
    "y = [  1,   5,  56, 109,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0,   0,   0]\n",
    "\n",
    "for i in y:\n",
    "    if i < 1:\n",
    "        break\n",
    "    print(idx_to_token[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnscp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d2e36f6420f9cedbd3e19077905bafcfb27b7f487fcc2faafdac026873e91f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
