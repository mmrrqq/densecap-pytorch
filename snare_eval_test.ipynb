{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from model.densecap import densecap_resnet50_fpn, DenseCapModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_config_path: Path, checkpoint_path: Path, return_features=False, box_per_img=50, verbose=False, token_to_idx=None):\n",
    "    with open(model_config_path, 'r') as f:\n",
    "        model_args = json.load(f)\n",
    "\n",
    "    model = densecap_resnet50_fpn(backbone_pretrained=model_args['backbone_pretrained'],\n",
    "                                  return_features=return_features,\n",
    "                                  feat_size=model_args['feat_size'],\n",
    "                                  hidden_size=model_args['hidden_size'],\n",
    "                                  max_len=model_args['max_len'],\n",
    "                                  emb_size=model_args['emb_size'],\n",
    "                                  rnn_num_layers=model_args['rnn_num_layers'],\n",
    "                                  vocab_size=model_args['vocab_size'],\n",
    "                                  fusion_type=model_args['fusion_type'],\n",
    "                                  box_detections_per_img=box_per_img,\n",
    "                                  token_to_idx=token_to_idx)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    if verbose and 'results_on_val' in checkpoint.keys():\n",
    "        print('[INFO]: checkpoint {} loaded'.format(checkpoint_path))\n",
    "        print('[INFO]: correspond performance on val set:')\n",
    "        for k, v in checkpoint['results_on_val'].items():\n",
    "            if not isinstance(v, dict):\n",
    "                print('        {}: {:.3f}'.format(k, v))\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_image_paths(parent_folder: Path) -> List[str]:\n",
    "    image_paths = []\n",
    "\n",
    "    for child in parent_folder.iterdir():\n",
    "        if child.is_dir():\n",
    "            image_paths.extend(get_image_paths(child))\n",
    "            continue\n",
    "        image_paths.append(str(child))\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def img_to_tensor(img_list, device):\n",
    "    img_tensors = []\n",
    "\n",
    "    for img_path in img_list:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensors.append(tf.ToTensor()(img).to(device))\n",
    "\n",
    "    return img_tensors\n",
    "\n",
    "\n",
    "def describe_images(model: DenseCapModel, img_list: List[str], device: torch.device):\n",
    "    all_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        for i in tqdm(range(0, len(img_list), BATCH_SIZE)):\n",
    "            image_tensors = img_to_tensor(img_list[i:i+BATCH_SIZE], device=device)\n",
    "\n",
    "            results = model(image_tensors)\n",
    "\n",
    "            all_results.extend([{k:v.cpu() for k,v in r.items()} for r in results])\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_results(results, img_paths: List[str], idx_to_token):\n",
    "    results_dict = {}\n",
    "\n",
    "    for img_path, result in zip(img_paths, results):\n",
    "        results_dict[img_path] = []        \n",
    "\n",
    "        for box, cap, score in zip(result['boxes'], result['caps'], result['scores']):            \n",
    "            r = {\n",
    "                'box': [round(c, 2) for c in box.tolist()],\n",
    "                'score': round(score.item(), 2),\n",
    "                'cap': ' '.join(idx_to_token[idx] for idx in cap.tolist()\n",
    "                                if idx_to_token[idx] not in ['<pad>', '<bos>', '<eos>'])\n",
    "            }            \n",
    "\n",
    "            results_dict[img_path].append(r)\n",
    "\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_result(image_file_path, result, idx_to_token=None):\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "    assert isinstance(result, list)\n",
    "\n",
    "    img = Image.open(image_file_path)\n",
    "    plt.imshow(img)\n",
    "    ax = plt.gca()\n",
    "    N = 0\n",
    "    for r in result:        \n",
    "        if N > 5:\n",
    "            break\n",
    "        \n",
    "        if idx_to_token is not None:\n",
    "            r['cap'] = ' '.join(idx_to_token[idx] for idx in r['cap'].tolist() if idx_to_token[idx] not in ['<pad>', '<bos>', '<eos>'])        \n",
    "        \n",
    "        if \"car\" not in r['cap']:\n",
    "            continue\n",
    "\n",
    "        N += 1\n",
    "\n",
    "        ax.add_patch(Rectangle((r['box'][0], r['box'][1]),\n",
    "                               r['box'][2]-r['box'][0],\n",
    "                               r['box'][3]-r['box'][1],\n",
    "                               fill=False,\n",
    "                               edgecolor='red',\n",
    "                               linewidth=3))\n",
    "        ax.text(r['box'][0], r['box'][1], r['cap'] + (r['view'] if 'view' in r else \"\"), style='italic', bbox={'facecolor':'white', 'alpha':0.7, 'pad':10})\n",
    "    fig = plt.gcf()\n",
    "    plt.tick_params(labelbottom='off', labelleft='off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decode_caption(cap: torch.Tensor, idx_to_token):\n",
    "    for i in cap:\n",
    "        if i < 1:\n",
    "            break\n",
    "        print(idx_to_token[i.item()], end=\" \")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: checkpoint compute_model_params/without_aux.pth.tar loaded\n",
      "[INFO]: correspond performance on val set:\n",
      "        map: 0.108\n",
      "        detmap: 0.264\n",
      "../snare/amt/folds_adversarial/train.json\n",
      "Loaded Entries. train: 39278 entries\n",
      "../snare/amt/folds_adversarial/test.json\n",
      "Loaded Entries. test: 8751 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4376 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(test_loader):    \n\u001b[1;32m     37\u001b[0m     (key1_imgs, key2_imgs), gt_idx, (key1, key2), annotation, is_visual \u001b[39m=\u001b[39m batch \n\u001b[0;32m---> 39\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_visual:\n\u001b[1;32m     40\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mskip non visual\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "from utils.snare_dataset import SnareDataset\n",
    "\n",
    "\n",
    "lut_path = Path(\"./data/VG-regions-dicts-lite.pkl\")\n",
    "\n",
    "with open(lut_path, 'rb') as f:\n",
    "    look_up_tables = pickle.load(f)\n",
    "\n",
    "idx_to_token = look_up_tables['idx_to_token']\n",
    "token_to_idx = look_up_tables['token_to_idx']\n",
    "\n",
    "params_path = Path(\"compute_model_params\")\n",
    "model_name = \"without_aux\"\n",
    "model = load_model(\n",
    "    params_path / model_name / \"config.json\", \n",
    "    params_path / (model_name + \".pth.tar\"), \n",
    "    return_features=False, verbose=True, token_to_idx=token_to_idx)\n",
    "\n",
    "\n",
    "dataset = SnareDataset(mode=\"train\")\n",
    "test_set = SnareDataset(mode=\"test\")\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_set)\n",
    "\n",
    "view_ids = torch.arange(8)\n",
    "model.rpn.training = False\n",
    "\n",
    "n = 0\n",
    "pos = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# model.rpn.training = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):    \n",
    "        (key1_imgs, key2_imgs), gt_idx, (key1, key2), annotation, is_visual = batch \n",
    "\n",
    "        if np.sum(is_visual) < 1:\n",
    "            continue\n",
    "\n",
    "        key1_imgs = key1_imgs[is_visual]\n",
    "        key2_imgs = key2_imgs[is_visual]        \n",
    "\n",
    "        key1_imgs = [k.squeeze() for k in key1_imgs]\n",
    "        key2_imgs = [k.squeeze() for k in key2_imgs]\n",
    "        annotation = annotation[0]\n",
    "        \n",
    "        if gt_idx > 0:\n",
    "            key1_imgs, key2_imgs = key2_imgs, key1_imgs\n",
    "                \n",
    "        losses1, min_loss_cap1 = model.query_caption(key1_imgs, [annotation], view_ids)        \n",
    "        losses2, min_loss_cap2 = model.query_caption(key2_imgs, [annotation], view_ids)                \n",
    "\n",
    "\n",
    "        print(f\"gt annot: {annotation}\")\n",
    "        n += 1\n",
    "        if losses2['caption_min'] > losses1['caption_min']:\n",
    "            pos += 1\n",
    "            print(\"correct!\")\n",
    "            print_decode_caption(min_loss_cap1, idx_to_token)        \n",
    "        else:\n",
    "            print_decode_caption(min_loss_cap2, idx_to_token)\n",
    "\n",
    "        print(f\"{pos/n:2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnscp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d2e36f6420f9cedbd3e19077905bafcfb27b7f487fcc2faafdac026873e91f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
